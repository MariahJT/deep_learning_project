{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement the CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#                       Setup python environment                     #\n",
    "######################################################################\n",
    "# !pip install torch torchvision\n",
    "# conda update pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))\n",
    "\n",
    "\n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           RESNET\n",
    "##############################\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            # Pads the input tensor using the reflection of the input boundary.\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the architecture of the generator network.\n",
    "       Note: Both generators G_AtoB and G_BtoA have the same architecture in this assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        in_channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block (first convolutional layer)\n",
    "        out_features = 64 \n",
    "        model = [\n",
    "            # Pads the input tensor using the reflection of the input boundary.\n",
    "            nn.ReflectionPad2d(in_channels), \n",
    "            # Conv2d(in_channels, out_channels, kernel_size=7, stride=1, padding=0)\n",
    "            nn.Conv2d(in_channels, out_features, 7), \n",
    "            # Applies Instance Normalization over a 4D input\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            print(out_features)\n",
    "            model += [\n",
    "                # Upsamples a given multi-channel data. Double the spatial space.\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        # we should make sure that the result of the generator have the same size as of the input image\n",
    "        model += [nn.ReflectionPad2d(in_channels), nn.Conv2d(out_features, in_channels, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generates an image conditioned on an input image.\n",
    "\n",
    "            Input\n",
    "            -----\n",
    "                x: BatchSize x 3 x 16 x 16\n",
    "\n",
    "            Output\n",
    "            ------\n",
    "                out: BatchSize x 3 x 16 x 16\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        out_size = self.model(x).size()\n",
    "        if out_size != torch.Size([batch_size, 3, 16, 16]):\n",
    "            raise ValueError(\"expect {} x 3 x 16 x 16, but get {}\".format(batch_size, out_size))\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the architecture of the discriminator network.\n",
    "       Note: Both discriminators D_A and D_B have the same architecture in this assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=100, channels=3, checkpoint_interval=-1, dataset_name='MNIST2USPS', decay_epoch=20, epoch=0, img_height=16, img_size=16, img_width=16, lambda_cyc=10.0, lambda_id=5.0, lr=0.0002, n_cpu=8, n_epochs=50, n_residual_blocks=9, sample_interval=10)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"MNIST2USPS\", help=\"name of the dataset\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=100, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--decay_epoch\", type=int, default=20, help=\"epoch from which to start lr decay\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--img_height\", type=int, default=16, help=\"size of image height\")\n",
    "parser.add_argument(\"--img_width\", type=int, default=16, help=\"size of image width\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=16, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=10, help=\"interval between saving generator outputs\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int, default=-1, help=\"interval between saving model checkpoints\")\n",
    "parser.add_argument(\"--n_residual_blocks\", type=int, default=9, help=\"number of residual blocks in generator\")\n",
    "parser.add_argument(\"--lambda_cyc\", type=float, default=10.0, help=\"cycle loss weight\")\n",
    "parser.add_argument(\"--lambda_id\", type=float, default=5.0, help=\"identity loss weight\")\n",
    "opt = parser.parse_args(\"\")\n",
    "\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def to_rgb(image_tensor):\n",
    "    trans = transforms.ToPILImage()\n",
    "    pil_img = trans(image_tensor)\n",
    "    # rgb_image = rgb_image.convert('RGB')\n",
    "    rgb_image = Image.new(\"RGB\", pil_img.size)\n",
    "    rgb_image.paste(pil_img)\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "class MergedDataset(Dataset):\n",
    "    '''\n",
    "    The MergedDataset class convert grayscale images into 3 channel RGB images\n",
    "    And merges two datasets of different styles into one dataset.\n",
    "    '''\n",
    "    def __init__(self, dataset_A, dataset_B, transforms_=None, unaligned=False, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        self.dataset_A = dataset_A\n",
    "        self.dataset_B = dataset_B\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = self.dataset_A[index % len(self.dataset_A)][0]\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_B = self.dataset_B[random.randint(0, len(self.dataset_B) - 1)][0]\n",
    "        else:\n",
    "            image_B = self.dataset_B[index % len(self.dataset_B)][0]\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        image_A = to_rgb(image_A)\n",
    "        image_B = to_rgb(image_B)\n",
    "        \n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.dataset_A), len(self.dataset_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Translation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "\n",
    "MNIST_trainset = datasets.MNIST(\n",
    "                    \"../../data/mnist\",\n",
    "                    train=True,\n",
    "                    download=True,\n",
    "                    transform=transforms.Compose(\n",
    "                        [transforms.Resize(opt.img_size), transforms.ToTensor()] \n",
    "                        # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                        # We used the mean and std of mnist dataset to normalize the data\n",
    "                        # However it doesn't work well.\n",
    "                    ),\n",
    "                )\n",
    "MNIST_testset = datasets.MNIST(\n",
    "                    \"../../data/mnist\",\n",
    "                    train=False,\n",
    "                    download=True,\n",
    "                    transform=transforms.Compose(\n",
    "                        [transforms.Resize(opt.img_size), transforms.ToTensor()] \n",
    "                    ),\n",
    "                )\n",
    "mnist_trainloader = DataLoader(MNIST_trainset, batch_size=opt.batch_size, shuffle=True)\n",
    "mnist_testloader = DataLoader(MNIST_testset, batch_size= opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "torch.Size([1, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'An Example from MNIST Dataset: Ground Truth is 7')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_mnist = enumerate(mnist_testloader)\n",
    "batch_idx_mnist, (example_data_mnist, example_targets_mnist) = next(examples_mnist)\n",
    "print(example_data_mnist.shape)\n",
    "print(example_data_mnist[0].shape)\n",
    "\n",
    "# plot an example from the dataloader\n",
    "plt.imshow(example_data_mnist[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"An Example from MNIST Dataset: Ground Truth is {}\".format(example_targets_mnist[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('./data/usps.h5', 'r') as hf:\n",
    "    train = hf.get('train')\n",
    "    X_tr = train.get('data')[:]\n",
    "    y_tr = train.get('target')[:].astype(int)\n",
    "    test = hf.get('test')\n",
    "    X_te = test.get('data')[:]\n",
    "    y_te = test.get('target')[:].astype(int)\n",
    "\n",
    "# The data were in an 1D numpy array of grayscale and normalised between [0,1]\n",
    "# We transform the data into the same dimensions as for the mnist.\n",
    "X_tr = X_tr.reshape(-1,1,16,16) #*255 \n",
    "X_te = X_te.reshape(-1,1,16,16) #*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPS_trainset = data.TensorDataset(torch.Tensor(X_tr),torch.Tensor(y_tr))\n",
    "USPS_testset = data.TensorDataset(torch.Tensor(X_te),torch.Tensor(y_te)) \n",
    "\n",
    "usps_trainloader = DataLoader(USPS_trainset, batch_size=opt.batch_size, \n",
    "                              shuffle=True, num_workers=opt.n_cpu)\n",
    "usps_testloader = DataLoader(USPS_testset, batch_size=opt.batch_size, \n",
    "                             shuffle=False, num_workers=opt.n_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "torch.Size([1, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'An Example from USPS Dataset: Ground Truth is 9.0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEICAYAAADMRzbSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAb+klEQVR4nO3de7xcVX338c+XXJQAacCAIQQSVKRSFcWIFC/wAkTEcHvap3JRosXmwefRasUHKLSCVCwoxQtYNSqCkcYKKkZEG4oiqFwMFJAYLoFiEm7hYm4QNAm//rHWwcmw5syZnXPmlu/79ZrXmdl7rVm/2ZffrL32PnsUEZiZ2ca26HQAZmbdyMnRzKzAydHMrMDJ0cyswMnRzKzAydHMrMDJsUDSxZI+UbHuGyXdK2mNpCOHOzazoZD0Hkk/70C70ySFpNFDLP8lSf840nFVMeTkKOlaSb+T9IKqjdUsuDV1j3dWfc8udBZwYURsHRFXtLNhSftLWlaYfq2k9+XnEyRdJOkRSasl3SPplJqyIempvF4elHS+pFF53psk/VLSSklPSvqFpNc3iOVMSetyGwPtXChpxxY+z3Nxj6Qq7UgaK+ljku7Oy+tBST+SdPBIxTkcJL25Zr97qrA/7tLi+z0g6aCq8UTEiRHxT63Wk7SvpJvztnWHpDcNUlaSzpX0RH58SpKatTGk5ChpGvBmIIDDhxj/YCbk5DHw+PdheM9uMRVYWJqRV1Kne+ufAbYGXgH8CWl93ldXZs+I2Bo4EDgW+BtJ44ErgQuA7YCdgI8Dvx+krX+PiG1y+aOAScAtrSTILnY5cARwPLAtsCvwOeAdpcJD7UmNtIi4fmC/A/4sT67dH5cMlO2WmOtJ2g6YB3wamAB8CviBpG0bVJkFHAnsCbwamAH8n6YNRUTTB/Ax4BfA+cCVdfMuBr4A/BBYDdwEvLTB+0wjJdjRhXljgduAD+bXo3KbH8uv9wZuAFYADwMXAmNr6gfwf4F7cxz/BLw011kFfHugPLA/sAw4DXgceAA4ru4zfaLm9Ywc2wrgl8CrG3y++4BngbXAGuAFwLXA2fmzrAVeBuwL/ApYmf/uW/Me1wKfyO2sAX4AvAi4NH+OXwHTGrS/P7CsMP1a4H35+Z3AkYOs6wBeVvP6sryspwMrhrK95HpnAt+smzYKuB04L7/elpRwHwN+l59PyfPOBjYAz+TlcGGe/jlgaV4WtwBvrnn/vYEFed6jwPk18/bJy3RFjmH/wdpp8tkOyutySpNyDwCnAHeQvkRGk76MFuY4rgVeMciyf2475I/b7EnActI+8N6asi8iJYxVwM2k7f/nTeKbRs3+mNfZ5cA38/u8j+fvC89tY8AcNt7eT655z5nAEtL+dfogMdR+xol5G1gBPAlcD2xRqDMDWFg37R7ghAZt/BKYVfP6BODGput5iBv6YlLieR2wDnhx3Yd7Mm+Yo0k78beGsjIK819J2kleAZwO3AiMyvNeR9rAR+f3WQR8uG7DmgeMJ30j/h64BngJqYf0G2BmzQpeT0r2LwD2A54Cdi+ssL3yxvgG0s49k7TRv2CQHeKgusS0JMc0Gnhx/ozvzq+Pya9fVFN+MSmxD8R9D2mHHA18A/h6g7b3p3ly/Cpp53wvsFuh7HM7KLAH8EjemMYDTwCXAG8Htm2yzZxJXXLM088CbqrZof8CGAdsQ0rEV5Tirpn2rlxvNClRPAK8MM+7AXh3fr41sE9+vlOO/VDS0dJb8+vtB2nnSuDUBp/tHODaIew3D5C+VHcGtgRenreztwJjSMlkMX/80m6WHNfn5Tcmf5anB9YD8C1SB2Ar0n70INWS4zpSL2uLHPNzMZS2MZ6/vQ+851dy/T1J++IrGsRQ+xn/GfhS/nxjSEerKtQ5DPhN3bR7gc80aGMl8Iaa19OB1c3WX9NDvHwsPxX4dkTcQuodHVtX7LsRcXNErCclx9c0edvHJa2oebwCICLuJPWavgd8lLShb8jzbomIGyNifUQ8AHyZlNRqnRsRqyJiIamHND8i7o+IlcCPgNfWlf/HiPh9RPyM1PP9q0KsfwN8OSJuiogNEXEJaWXv0+Qz1ro4Ihbm5XMwcG9EzMmfZS5wF2mFD/h6RNxXE/d9EfGfuf5lhc/Rig+S1tEHgN9IWizp7XVlbpX0O1Kv9as5nlXAm/jjhv+YpHmSXtxi+w+RDrOJiCci4jsR8XRErCb14urX6UYi4pu53vqI+BfSl9vuefY64GWSJkbEmoi4MU9/F3BVRFwVEc9GxNWkHuahg7QzIyLOaTB7IikpA+kwL2/HKyU9U1f28xGxNCLWAu8EfhgRV0fEOuA8UgLZd7DPXGMdcFZErIuIq0i9td3zmPBfkI6ynsr70SVDfM96N0TEFXk5ra34HgAfj4i1EXE7qae+5xDqrAN2BKbmz3h95GxW55fAZEnHSBojaSapMzGuwftuTUqQA1YCWzcbdxzK+NdMUpJ5PL/+tzyt1iM1z5/OwQxmYkRMqHksqpl3Cenb56qIuHdgoqSXS7oyn0hYBXyStJHWerTm+drC69q4fhcRT9W8/i0wuRDrVOCk2mRO6gmUyjaytOb55NxWrd+SejcDWvkctdaTvnHrjSFteOQN9pMR8TpSD+zbwGV5HGfAXhGxbUS8NCL+ISKezXUXRcR7ImIKqXcyGfhsg1ga2Yl0pIGkcZK+LOm3eZ1eB0wYOAFUIukkSYtyIlpB6l0PbAcnkHpnd0n6laQZefpU4H/XrcM3kXbEKp6orRsRT0bEBNLRTf0Jy4brPi/XpWy87gdtN39BDhjY17Yn9aRr26rfxoZqafMiQ9JqToA0hrgYmC/pfkmnlgpFxBOk8d6PkPaNQ4D/JA07lKwhHfkMGA+saZB4nzNocpS0Jak3tV9OSo8AfwfsKWko3wRV/CvpkOZtdWegvkjqYe0WEeNJ44VNzzgNYltJW9W83oXUq6m3FDi7LpmPyz2+oapdCQ+RdtZau5AOgzbVEmCipOc2xPztOJXCzpJ7g58kHYrt2kpDEXEX6ZDolUOtk09GHUYaS4J0WLw76ZBnPPCWgaIDzdTVfzNpDO+vSIeTE0i9AOWY7o2IY4AdgHOBy/M6XgrMqVuHW9X0DFu9NdU1wOslTRlC2YbrPq+bnfnjun+ajXs/k4YYz2OkL8ada6a1dNa5Rv2yeKpJTMN2W6+IWB0RJ0XES0jbyUckHdig7M8i4vURsR1piGp30lhryUI27rnuSYOTprWa9RyPJA1W70E6VH4NaTzwetJZumEl6d2kb9/3AH8LXFKzo29DGiReI+lPgfcPQ5Mfz5dkvJk0yHtZocxXgBMlvSGfbd5K0jskbVOxzauAl0s6VtJopcuY9iB9IWySSGcabwLOlbS10mVX/5+049wIIOkfJb0+f+4XAh8iDYDfPdh7S/rT3Gubkl/vTBovvXGwernsmDx0Mpe0c52fZ21D6gmvyD3XM+qqPkoaM6am/HpSMhgt6WPU9AgkvUvS9rlHtiJP3kA6wXCYpLdJGiXphUqXPQ0kt/p2BhUR84GfAlfk7WKspDE0H2r5NvAOSQfm8ieRhmh+meffBhybYzyEJkMMNfFsAL4LnJl743vw/KO7qm4DDs1DB5OAD9fNb2nZDUbSDEkvy18aq0jrbkODsq/N29V40vDEsoj4jwZv/Q1Sot1J0mTScr+4WTzNkuNM0njTkoh4ZOBBOnt53Cac6l+hja+t+ojS9VWfBY7P40X/RhoX+kyu81HSWOdqUsLa1Mt/HiGdCHmINAZ3Yu4NbSQiFpDGHS/M5ReTkncl+ZBgBmkFPUEalJ9RM2yxqd5J6jktJvVIDgQOjYiBsbAAvk46i/gQ6eTAOyJiTZP3XU06KXWTpKdISfHO/DkaxiJpDSlRzSN93tdFxEAP/bOkMbfH8/v9uK7+54C/VLq+9vPAf5DGYO8h9YSfYePDwEOAhbnNzwFHR8QzEbGUdBh2GimxLiV9aWzRoB2Urlk8bZDP9r9IX2jfzJ/vv4HjcgxFEXE3afzzgvyZDwMOi4g/5CIfytNW5Pdq5TrZD5AOXR8h7fhfb6HuYOaQxgwfAObz/P3un4F/yMMVH93EtnYjHR6vIZ1c+9eIuLZB2ZNJy3ApaYjjqIEZytdy1pT9Mmn8/NekbfaHedqg1OSwuy9J2p90JnUoh0Vmthnq9AXJZmZdycnRzKxgszysNjNrxj1HM7OCrvzH8uEiyd3iGk3+IaChceMa/eNBY9ttt13zQgVjx46tVG/8+PHNC9VZsWJF80IFy5Y1uta4sXXr1lVqqxdExKZcb9y1+jo52sbGjCn980xzr3rVq1quc+yx9f9hOjRTp9ZfHz80BxxwQMt15s2bV6mtk08+ueU6Dz44HNf4Wzv5sNrMrMDJ0cysoKeSo6RDlO68vLjRP6WbmQ2HnkmO+U4tXyDdS3AP4Jj8P6RmZsOuZ5Ij6Wa6i/P9Gf9AurnnER2Oycz6VC8lx53Y+CYDyyjcB0/SLEkLJC1oW2Rm1nd66VKe0rVUz7uOMSJmA7PB1zmaWXW91HNcxsY385xC+ea0ZmabrJeS46+A3STtKmkscDTpHoFmZsOuZw6rI2K9pA+Qbng6Crgo0g9pmZkNu55JjgD5F9eu6nQcZtb/eumw2sysbfr6fo79erZ62rRplepddlnp98Oamz59eqV6VVTdHteubf0nlqvcbQjgmWfqf5q6uRkzZjQvVHDNNddUqtdO/XpXHvcczcwKnBzNzAqcHM3MCpwczcwKnBzNzAqcHM3MCpwczcwKnBzNzAqcHM3MCpwczcwKnBzNzAqcHM3MCnzjiQ7baafn/QxOUzfffHOltiZPnlyp3v33399ynfPOO69SWz/5yU8q1VuzZk3LdbbffvtKbc2fP7/lOk8//XSltvbee+9K9ZYvX16pXhW+8YSZ2WbEydHMrMDJ0cysoGeSo6SdJf1U0iJJCyV9qNMxmVn/6qXfkFkPnBQRt0raBrhF0tUR8ZtOB2Zm/adneo4R8XBE3JqfrwYWAa2f6jUzG4Je6jk+R9I04LXATYV5s4BZbQ7JzPpMzyVHSVsD3wE+HBGr6udHxGxgdi7b9dc5mll36pnDagBJY0iJ8dKI+G6n4zGz/tUzyVGSgK8BiyLi/E7HY2b9rWeSI/BG4N3AAZJuy49DOx2UmfWnnhlzjIifA335P5xm1n16qedoZtY2vivPMNlyyy0r1atyx5tJkyZVauuss86qVO+cc85puc7atWsrtdULDj/88JbrfP/736/U1pw5cyrVO/744yvVq8J35TEz24w4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkV9Mwty7rdqaeeWqlelZtIXHzxxZXaOuOMMyrV61czZsyoVO+4444b5kgaO+igg9rWlm3MPUczswInRzOzAidHM7OCnkuOkkZJ+i9JV3Y6FjPrXz2XHIEPAYs6HYSZ9beeSo6SpgDvAL7a6VjMrL/1VHIEPgucDDzb6UDMrL/1THKUNANYHhG3NCk3S9ICSQvaFJqZ9aGeSY7AG4HDJT0AfAs4QNI36wtFxOyImB4R09sdoJn1j55JjhHx9xExJSKmAUcDP4mId3U4LDPrUz2THM3M2qkn/7c6Iq4Fru1wGGbWx9xzNDMr6Mme40jbYovWvzOOOuqoEYik7Nxzz21bW71i8uTJLde54oorKrU1atSoSvWquOWWQS/OaKhKjBs2bKjUVr9yz9HMrMDJ0cyswMnRzKzAydHMrMDJ0cyswMnRzKzAydHMrMDJ0cyswMnRzKzAydHMrMDJ0cyswMnRzKzAydHMrMB35Sl49tnWf79r0qRJldpavXp1y3Xuv//+Sm31gu23375SvVNOOaXlOu28u87cuXMr1TvxxBMr1fMddjade45mZgVOjmZmBU6OZmYFPZUcJU2QdLmkuyQtkvTnnY7JzPpTr52Q+Rzw44j4S0ljgXGdDsjM+lPPJEdJ44G3AO8BiIg/AH/oZExm1r966bD6JcBjwNcl/Zekr0raqr6QpFmSFkha0P4Qzaxf9FJyHA3sBXwxIl4LPAWcWl8oImZHxPSImN7uAM2sf/RSclwGLIuIm/Lry0nJ0sxs2PVMcoyIR4ClknbPkw4EftPBkMysj/XMCZnsg8Cl+Uz1/cB7OxyPmfWpnkqOEXEb4LFEMxtxPZUcu9n1119fqd4+++zTcp3x48dXauvxxx+vVO9tb3tby3UOOOCASm29973VDgaq3rCiinvuuaflOu9///srtbVq1apK9WzT9cyYo5lZOzk5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFfiuPMNkzpw5leoddthhLdd57LHHKrXVC9asWdPpEJo6++yzW66zcuXKEYjERpJ7jmZmBU6OZmYFTo5mZgU9lRwl/Z2khZLulDRX0gs7HZOZ9aeeSY6SdgL+FpgeEa8ERgFHdzYqM+tXPZMcs9HAlpJGA+OAhzocj5n1qZ5JjhHxIHAesAR4GFgZEfPry0maJWmBpAXtjtHM+kfPJEdJ2wJHALsCk4GtJL2rvlxEzI6I6RHhn3A1s8p6JjkCBwH/HRGPRcQ64LvAvh2Oycz6VC8lxyXAPpLGSRJwILCowzGZWZ/qmeQYETcBlwO3Ar8mxT67o0GZWd/qqf+tjogzgDM6HYeZ9b+e6TmambWTIqLTMYwYSV3/4fbbb7+W65x22mmV2po6dWqlelXMnTu3Ur0lS5ZUqnfRRRe1XKfqnXL22GOPlus89FD/XpIbEep0DCPBPUczswInRzOzAidHM7MCJ0czswInRzOzAidHM7MCJ0czswInRzOzAidHM7MCJ0czswInRzOzAidHM7OCnrplWT/62c9+1pY6veKCCy5oW1s33HBDpXr9fBMJ+yP3HM3MCpwczcwKnBzNzAq6LjlKukjSckl31kzbTtLVku7Nf7ftZIxm1v+6LjkCFwOH1E07FbgmInYDrsmvzcxGTNclx4i4DniybvIRwCX5+SXAkW0Nysw2O71yKc+LI+JhgIh4WNIOjQpKmgXMaltkZtaXeiU5DllEzCb/nnUv/MCWmXWnrjusbuBRSTsC5L/LOxyPmfW5XkmO84CZ+flM4PsdjMXMNgNdlxwlzQVuAHaXtEzSCcA5wFsl3Qu8Nb82MxsxXTfmGBHHNJh1YFsDMbPNWtf1HM3MukHX9RytP0iqVO/ggw8e5kga+8UvftG2tqz3uOdoZlbg5GhmVuDkaGZW4ORoZlbg5GhmVuDkaGZW4ORoZlbg5GhmVuDkaGZW4ORoZlbg5GhmVuDkaGZW4BtP2IiYOHFipXq77rrrMEfS2A9+8IO2tWW9xz1HM7MCJ0czswInRzOzgq5LjpIukrRc0p010z4t6S5Jd0j6nqQJnYzRzPpf1yVH4GLgkLppVwOvjIhXA/cAf9/uoMxs89J1yTEirgOerJs2PyLW55c3AlPaHpiZbVa6LjkOwV8DP2o0U9IsSQskLWhjTGbWZ3rqOkdJpwPrgUsblYmI2cDsXD7aFJqZ9ZmeSY6SZgIzgAMjwknPzEZUTyRHSYcApwD7RcTTnY7HzPpf1405SpoL3ADsLmmZpBOAC4FtgKsl3SbpSx0N0sz6Xtf1HCPimMLkr7U9EDPbrHVdz9HMrBt0Xc/R+sMuu+xSqd6YMWMq1Xv88cdbrnP33XdXass2D+45mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkVODmamRU4OZqZFTg5mpkV+K48NiJ22GGHtra3atWqlus888wzIxCJ9Qv3HM3MCpwczcwKui45SrpI0nJJdxbmfVRSSJrYidjMbPPRdckRuBg4pH6ipJ2BtwJL2h2QmW1+ui45RsR1wJOFWZ8BTgb8m9VmNuJ64my1pMOBByPidknNys4CZrUlMDPrW12fHCWNA04HDh5K+YiYDczOdd3LNLNKuu6wuuClwK7A7ZIeAKYAt0qa1NGozKyvdX3PMSJ+DTx3RXFOkNMjovXf4jQzG6Ku6zlKmgvcAOwuaZmkEzodk5ltfrqu5xgRxzSZP61NoZjZZqzreo5mZt2g63qO1h+uu+66SvV+/OMfV6o3aVLr5+dGjRpVqa0NGzZUqme9xT1HM7MCJ0czswInRzOzAidHM7MCJ0czswInRzOzAidHM7MCJ0czswInRzOzAidHM7MCJ0czswInRzOzAidHM7MCRfTvz6xIegz4bYPZE4FuuJu449iY49hYt8cxNSK2b3cw7dDXyXEwkhZExHTH4TgcR+/E0U4+rDYzK3ByNDMr2JyT4+xOB5A5jo05jo05jg7ZbMcczcwGszn3HM3MGnJyNDMr6OvkKOkQSXdLWizp1MJ8Sfp8nn+HpL1GIIadJf1U0iJJCyV9qFBmf0krJd2WHx8b7jhyOw9I+nVuY0FhfjuWx+41n/M2SaskfbiuzIgtD0kXSVou6c6aadtJulrSvfnvtg3qDro9DUMcn5Z0V17235M0oUHdQdfjMMRxpqQHa5b/oQ3qDtvy6EoR0ZcPYBRwH/ASYCxwO7BHXZlDgR8BAvYBbhqBOHYE9srPtwHuKcSxP3BlG5bJA8DEQeaP+PIorKNHSBcSt2V5AG8B9gLurJn2KeDU/PxU4Nwq29MwxHEwMDo/P7cUx1DW4zDEcSbw0SGsu2FbHt346Oee497A4oi4PyL+AHwLOKKuzBHANyK5EZggacfhDCIiHo6IW/Pz1cAiYKfhbGMYjfjyqHMgcF9ENPovpmEXEdcBT9ZNPgK4JD+/BDiyUHUo29MmxRER8yNifX55IzCl6vtvShxDNKzLoxv1c3LcCVha83oZz09KQykzbCRNA14L3FSY/eeSbpf0I0l/NkIhBDBf0i2SZhXmt3V5AEcDcxvMa8fyGPDiiHgY0pcZsEOhTLuXzV+TevElzdbjcPhAPry/qMEwQ7uXR9v1c3JUYVr9dUtDKTMsJG0NfAf4cESsqpt9K+nQck/gAuCKkYgBeGNE7AW8Hfh/kt5SH2ahzkgtj7HA4cBlhdntWh6taOeyOR1YD1zaoEiz9bipvgi8FHgN8DDwL6UwC9P66rrAfk6Oy4Cda15PAR6qUGaTSRpDSoyXRsR36+dHxKqIWJOfXwWMkTRxuOOIiIfy3+XA90iHRrXasjyytwO3RsSjhTjbsjxqPDowfJD/Li+Uade2MhOYARwXeXCv3hDW4yaJiEcjYkNEPAt8pcH7t3Nb6Yh+To6/AnaTtGvupRwNzKsrMw84Pp+l3QdYOXB4NVwkCfgasCgizm9QZlIuh6S9SevliWGOYytJ2ww8Jw3+31lXbMSXR41jaHBI3Y7lUWceMDM/nwl8v1BmKNvTJpF0CHAKcHhEPN2gzFDW46bGUTvOfFSD9x/x5dFxnT4jNJIP0tnXe0hn1U7P004ETszPBXwhz/81MH0EYngT6XDjDuC2/Di0Lo4PAAtJZ/xuBPYdgThekt//9txWR5ZHbmccKdn9Sc20tiwPUkJ+GFhH6v2cALwIuAa4N//dLpedDFw12PY0zHEsJo3jDWwnX6qPo9F6HOY45uT1fwcp4e040sujGx/+90Ezs4J+Pqw2M6vMydHMrMDJ0cyswMnRzKzAydHMrMDJ0cyswMnRzKzgfwCudIbn/oHWvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_usps = enumerate(usps_testloader)\n",
    "batch_idx_usps, (example_data_usps, example_targets_usps) = next(examples_usps)\n",
    "print(example_data_usps.shape)\n",
    "print(example_data_usps[0].shape)\n",
    "\n",
    "# plot an example from the dataloader\n",
    "plt.imshow(example_data_usps[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"An Example from USPS Dataset: Ground Truth is {}\".format(example_targets_usps[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(opt.img_height), Image.BICUBIC),\n",
    "    ## Add some noise into the data\n",
    "    transforms.RandomCrop((opt.img_height, opt.img_width)),\n",
    "    ## Random flip on image\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ## Normalise data\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    MergedDataset(MNIST_trainset, USPS_trainset, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0 #opt.n_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(dataloader)\n",
    "batch_idx, batch = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(batch['A'][0].transpose(1,2).transpose_(0,2))\n",
    "axes[0].set_title(\"RGB Image from MNIST Dataset\")\n",
    "\n",
    "axes[1].imshow(batch['B'][0].transpose(1,2).transpose_(0,2))\n",
    "axes[1].set_title(\"RGB Image from USPS Dataset\")\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "64\n",
      "128\n",
      "64\n",
      "[Epoch 0/50] [Batch 3/600] [D loss: 0.499400] [G loss: 7.874822, adv: 0.998800, cycle: 0.461459, identity: 0.452287] ETA: 38 days, 7:20:03.5153502"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e0975f7cd687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m# Cycle loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mrecov_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG_BtoA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mloss_cycle_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecov_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_A\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mrecov_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG_AtoB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-c334af69bfbe>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expect {} x 3 x 16 x 16, but get {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;31m##############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-c334af69bfbe>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create sample and checkpoint directories\n",
    "os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "input_shape = (opt.channels, opt.img_height, opt.img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AtoB = GeneratorResNet(input_shape, opt.n_residual_blocks)\n",
    "G_BtoA = GeneratorResNet(input_shape, opt.n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    G_AtoB = G_AtoB.cuda()\n",
    "    G_BtoA = G_BtoA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    G_AtoB.load_state_dict(torch.load(\"saved_models/%s/G_AtoB_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    G_BtoA.load_state_dict(torch.load(\"saved_models/%s/G_BtoA_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    G_AtoB.apply(weights_init_normal)\n",
    "    G_BtoA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AtoB.parameters(), G_BtoA.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Image transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(opt.img_height), Image.BICUBIC),\n",
    "    ## Add some noise into the data\n",
    "    transforms.RandomCrop((opt.img_height, opt.img_width)),\n",
    "    ## Random flip on image\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ## Normalise data\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    MergedDataset(MNIST_trainset, USPS_trainset, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0 # opt.n_cpu\n",
    ")\n",
    "\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    MergedDataset(MNIST_testset, USPS_testset, transforms_=transforms_, unaligned=True, mode=\"test\"),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=0 #1,\n",
    ")\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AtoB.eval()\n",
    "    G_BtoA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AtoB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BtoA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), normalize=False)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        G_AtoB.train()\n",
    "        G_AtoB.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G_BtoA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AtoB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G_AtoB(real_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        fake_A = G_BtoA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "        \n",
    "        # Cycle loss\n",
    "        recov_A = G_BtoA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = G_AtoB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + opt.lambda_cyc * loss_cycle + opt.lambda_id * loss_identity\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator A\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator B\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                opt.n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_cycle.item(),\n",
    "                loss_identity.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(G_AtoB.state_dict(), \"saved_models/%s/G_AtoB_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(G_BtoA.state_dict(), \"saved_models/%s/G_BtoA_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (opt.dataset_name, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
